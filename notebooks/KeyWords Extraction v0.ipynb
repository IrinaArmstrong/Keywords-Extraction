{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number CPU: 12\n"
     ]
    }
   ],
   "source": [
    "# Basic\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from collections import defaultdict, Counter\n",
    "from typing import (List, Dict, Any, NoReturn, \n",
    "                    Tuple, Optional, Union)\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import multiprocessing\n",
    "from multiprocessing_logging import install_mp_handler\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Number CPU: {multiprocessing.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W2V\n",
    "import spacy\n",
    "import pymorphy2\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.phrases import Phrases\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim.models.phrases import Phrases, npmi_scorer\n",
    "from gensim.models import word2vec, keyedvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path(\"..\")\n",
    "DATA_DIR  = BASE_DIR / \"data\"\n",
    "LISTS_DIR  = BASE_DIR / \"lists\"\n",
    "MODEL_DIR  = BASE_DIR / \"models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c531869f755d49eb9fc8301a7a1a7673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data part of shape: (3944, 10)\n",
      "Data part of shape: (9200, 10)\n",
      "\n",
      "All data of shape: (13144, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>incident</th>\n",
       "      <th>req_reason</th>\n",
       "      <th>req_reg_datetime</th>\n",
       "      <th>req_num</th>\n",
       "      <th>msg</th>\n",
       "      <th>product</th>\n",
       "      <th>subproduct</th>\n",
       "      <th>subject</th>\n",
       "      <th>s_subject</th>\n",
       "      <th>day_of_the_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OTHER</td>\n",
       "      <td>Инцидент</td>\n",
       "      <td>2020-11-05 11:27:21</td>\n",
       "      <td>2011050726933001</td>\n",
       "      <td>здравствуйте здравствуйте я застрахованное лиц...</td>\n",
       "      <td>Страхование</td>\n",
       "      <td>.Страховой случай</td>\n",
       "      <td>Консультация по продуктам и обслуживанию</td>\n",
       "      <td>Разъяснения (условия, сроки, статусы рассмотре...</td>\n",
       "      <td>[310]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OTHER</td>\n",
       "      <td>Инцидент</td>\n",
       "      <td>2020-11-19 15:27:04</td>\n",
       "      <td>2011190004665301</td>\n",
       "      <td>Куда  #ТОПОНИМ  мой аватар Ну если не понял то...</td>\n",
       "      <td>Онлайн-сервисы</td>\n",
       "      <td>Функционирование МБ/СБОЛ/МП</td>\n",
       "      <td>Работа в системе</td>\n",
       "      <td>Информация по возможностям/ограничениям</td>\n",
       "      <td>[324]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OTHER</td>\n",
       "      <td>Инцидент</td>\n",
       "      <td>2020-11-20 13:16:35</td>\n",
       "      <td>2011200083620001</td>\n",
       "      <td>меня зовут  #ФИО  здравствуйте здравствуйте у ...</td>\n",
       "      <td>Физ. лица - иные услуги/продукты</td>\n",
       "      <td>Кредитная история (БКИ)</td>\n",
       "      <td>Вопросы по отчету</td>\n",
       "      <td>Отчет не поступил/поступил с ошибкой</td>\n",
       "      <td>[325]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OTHER</td>\n",
       "      <td>Инцидент</td>\n",
       "      <td>2020-11-21 19:29:17</td>\n",
       "      <td>2011210190303001</td>\n",
       "      <td>#ФИО  здравствуйте добрый день я вас слушаю я...</td>\n",
       "      <td>Онлайн-сервисы</td>\n",
       "      <td>Функционирование МБ/СБОЛ/МП</td>\n",
       "      <td>Работа в системе</td>\n",
       "      <td>Информация по возможностям/ограничениям</td>\n",
       "      <td>[326]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IM0104549077</td>\n",
       "      <td>Инцидент</td>\n",
       "      <td>2020-11-20 13:40:55</td>\n",
       "      <td>2011200086706501</td>\n",
       "      <td>меня зовут  #ФИО  здравствуйте  #ФИО  добрый д...</td>\n",
       "      <td>Онлайн-сервисы</td>\n",
       "      <td>Функционирование МБ/СБОЛ/МП</td>\n",
       "      <td>Работа в системе</td>\n",
       "      <td>Некорректная работа Мобильного Приложения СБОЛ</td>\n",
       "      <td>[325]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       incident req_reason     req_reg_datetime           req_num  \\\n",
       "0         OTHER   Инцидент  2020-11-05 11:27:21  2011050726933001   \n",
       "1         OTHER   Инцидент  2020-11-19 15:27:04  2011190004665301   \n",
       "2         OTHER   Инцидент  2020-11-20 13:16:35  2011200083620001   \n",
       "3         OTHER   Инцидент  2020-11-21 19:29:17  2011210190303001   \n",
       "4  IM0104549077   Инцидент  2020-11-20 13:40:55  2011200086706501   \n",
       "\n",
       "                                                 msg  \\\n",
       "0  здравствуйте здравствуйте я застрахованное лиц...   \n",
       "1  Куда  #ТОПОНИМ  мой аватар Ну если не понял то...   \n",
       "2  меня зовут  #ФИО  здравствуйте здравствуйте у ...   \n",
       "3   #ФИО  здравствуйте добрый день я вас слушаю я...   \n",
       "4  меня зовут  #ФИО  здравствуйте  #ФИО  добрый д...   \n",
       "\n",
       "                            product                   subproduct  \\\n",
       "0                       Страхование            .Страховой случай   \n",
       "1                    Онлайн-сервисы  Функционирование МБ/СБОЛ/МП   \n",
       "2  Физ. лица - иные услуги/продукты      Кредитная история (БКИ)   \n",
       "3                    Онлайн-сервисы  Функционирование МБ/СБОЛ/МП   \n",
       "4                    Онлайн-сервисы  Функционирование МБ/СБОЛ/МП   \n",
       "\n",
       "                                    subject  \\\n",
       "0  Консультация по продуктам и обслуживанию   \n",
       "1                          Работа в системе   \n",
       "2                         Вопросы по отчету   \n",
       "3                          Работа в системе   \n",
       "4                          Работа в системе   \n",
       "\n",
       "                                           s_subject day_of_the_year  \n",
       "0  Разъяснения (условия, сроки, статусы рассмотре...           [310]  \n",
       "1            Информация по возможностям/ограничениям           [324]  \n",
       "2               Отчет не поступил/поступил с ошибкой           [325]  \n",
       "3            Информация по возможностям/ограничениям           [326]  \n",
       "4     Некорректная работа Мобильного Приложения СБОЛ           [325]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = []\n",
    "for fn in tqdm_notebook(glob.glob(str(DATA_DIR / \"*.csv\"))):\n",
    "    df_part = pd.read_csv(fn, sep=';', encoding='utf-8')\n",
    "    print(f\"Data part of shape: {df_part.shape}\")\n",
    "    df.append(df_part)\n",
    "df = pd.concat(df).reset_index(drop=True)\n",
    "print(f\"All data of shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_opener(filename: str) -> str:\n",
    "    with open(filename, 'rt', encoding='utf-8-sig') as src:\n",
    "        file = src.read()\n",
    "    return \"|\".join([x for x in file.split('\\n') if x])\n",
    "\n",
    "\n",
    "class DataPreprocessorLemmatizer:\n",
    "\n",
    "    text_features = ['msg']\n",
    "    stopgrams = [\n",
    "             'CONJ',   # союз\n",
    "             'PRCL',   # частица\n",
    "             'PRED',   # предикатив\n",
    "             'NPRO',   # местоимение-сущ.\n",
    "             'INTJ',   # междометие\n",
    "             'Erro',   # ошибка\n",
    "             'Dist',   # искажение\n",
    "             'Ques',   # вопросительное слово\n",
    "             'Dmns',   # указательное слово\n",
    "             'Prnt'   # вводное слово\n",
    "            ]\n",
    "\n",
    "    def __init__(self, multipocess: bool, num_processors: int=16, chunksize: int=100, \n",
    "                intro_words_path: str = './lists'):\n",
    "        # Language parsers\n",
    "        self.__morph = pymorphy2.MorphAnalyzer()\n",
    "        self.__nlp = spacy.blank('ru')\n",
    "        # Multiprocessing params\n",
    "        self.__multipocess = multipocess\n",
    "        self.__num_processors = num_processors\n",
    "        self.__chunksize = chunksize\n",
    "        # Cleaning utils\n",
    "        self.__intro_words = file_opener(os.path.join(intro_words_path, 'intro.txt'))\n",
    "        self.__nltk_stopwords = file_opener(os.path.join(intro_words_path, 'NLTK_stopwords.txt')).split(\"|\")\n",
    "\n",
    "    def get_stopwords(self):\n",
    "        \"\"\" \n",
    "        Check intro-words list.\n",
    "        \"\"\"\n",
    "        return self.__nltk_stopwords\n",
    "\n",
    "    def get_intro_words(self):\n",
    "        \"\"\" \n",
    "        Check intro-words list.\n",
    "        \"\"\"\n",
    "        return self.__intro_words\n",
    "    \n",
    "    def get_analyzer(self):\n",
    "        \"\"\"\n",
    "        Allow to access to Pymorphy Analyzer instance.\n",
    "        \"\"\"\n",
    "        return self.__morph\n",
    "\n",
    "    \n",
    "    def _process_text(self, text: str):\n",
    "        \"\"\" \n",
    "        Process single text and return list of tokens.\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return []\n",
    "       # Pre-processing part \n",
    "        text = [str(token).lower()\n",
    "                for token in self.__nlp.make_doc(text)\n",
    "                if (token and token.is_alpha and len(str(token.text)) > 2 and ~token.is_stop)]\n",
    "        # Processing part\n",
    "        clean_text = []\n",
    "        for token in text:\n",
    "            token = self.__morph.parse(str(token).lower())[0]\n",
    "            if ((token.normal_form not in self.__nltk_stopwords) \n",
    "                and (token.normal_form not in self.__intro_words)\n",
    "                and all([tag not in token.tag for tag in self.stopgrams])):\n",
    "                clean_text.append(token.normal_form)\n",
    "        return clean_text\n",
    "    \n",
    "\n",
    "    def process_texts(self, texts: List[str]):\n",
    "        \"\"\" \n",
    "        Process list of texts and return list of lists of tokens.\n",
    "        \"\"\"\n",
    "        if self.__multipocess:\n",
    "            with multiprocessing.Pool(self.__num_processors) as pool:\n",
    "                processed_texts = list(tqdm_notebook(pool.imap(self._process_text, texts, \n",
    "                                                               chunksize=self.__chunksize), \n",
    "                                                     total=len(texts)))\n",
    "            return processed_texts\n",
    "        else:\n",
    "            return [self._process_text(text) for text in tqdm_notebook(texts)]\n",
    "        \n",
    "\n",
    "\n",
    "    def process(self, data: pd.DataFrame,\n",
    "                features_cols: Optional[List[str]] = None, copy: bool=True) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Preprocess text for language modelling.\n",
    "         - clean introduction words, numbers and small prefixes;\n",
    "         - tokenize and lemmatize texts;\n",
    "        \"\"\"\n",
    "        logging.info(\"Text processing started.\")\n",
    "        if not features_cols:\n",
    "            features_cols = self.text_features\n",
    "\n",
    "        for col_name in features_cols:\n",
    "            logging.info(f\"Processing '{col_name}' column...\")\n",
    "            data_processed = self.process_texts(data[col_name].fillna(\"\").to_list())\n",
    "            if not copy:\n",
    "                data[col_name] = data_processed\n",
    "            else:\n",
    "                data[col_name + \"_proc\"] = data_processed\n",
    "\n",
    "        logging.info(\"Text preprocessing finished.\")\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-16 20:25:39,195 - pymorphy2.opencorpora_dict.wrapper - INFO - Loading dictionaries from C:\\Users\\airen\\Anaconda3\\envs\\pycharmenv\\lib\\site-packages\\pymorphy2_dicts\\data\n",
      "2021-07-16 20:25:39,239 - pymorphy2.opencorpora_dict.wrapper - INFO - format: 2.4, revision: 393442, updated: 2015-01-17T16:03:56.586168\n"
     ]
    }
   ],
   "source": [
    "processor = DataPreprocessorLemmatizer(multipocess=False,\n",
    "                                       num_processors=8, chunksize=200,\n",
    "                                       intro_words_path=str(LISTS_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-16 20:25:42,439 - root - INFO - Text processing started.\n",
      "2021-07-16 20:25:42,440 - root - INFO - Processing 'msg' column...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaff9e9222334f42b525bb7b6a076fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13144.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-16 20:31:09,234 - root - INFO - Text preprocessing finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wall time: 5min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = processor.process(data=df, features_cols=['msg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "здравствуйте здравствуйте я застрахованное лицо я хотела бы узнать договор полиса и дата оплаты страховой премии мне по поводу кража у меня украли  #ЧИСЛО  тысяч мошенники я как бы чтоб страховой написать заявление то есть подать на страховой случай я правильно понимаю да да да мне должны указать договор полис я по телефону это все как то оплачивала договор вам оплатить нужно правильно понимаю нет у меня мошенники сняли  #ЧИСЛО  тысяч и как бы я хочу в страховую службу написать заявление мне нужно указать договор полис и дата оплаты страховой премии когда оплачивалась то есть вам нужен номер договора да номер договора мы не сообщаем как же мне заявление для по поводу кражи вы сказали вам звонить в сбербанк щас сижу так одна минута деньги  #ЧИСЛО  когда позвонит еще послан от обратиться этот номер договора номер договора говорит не сообщаем а потом и одну минуточку подождите не сообщаем а потом что мне делать и для документации не ладно сейчас минуточку подождите сейчас выясню я уже сказала уточните пожалуйста а где именно вам нужно указать номер договора заявление о наступлении события имеющие признаки страхового случая сбербанк страхование одна минута по техническим причинам отсутствует возможность предоставить интересующую вас информацию приносим извинения за доставленные неудобства приносим просим перезвони  #ТОПОНИМ  не поняла что то слово по техническим причинам отсутствует возможность предоставить интересующую вас информацию приносим извинения за доставленные неудобства и просим перезвонить вас позднее а что сбербанк честно говоря мне нужно час допустим заявление это но к сожалению нет сейчас технической возможности предоставить информацию в алло а если попозже перезвоню будет техническая возможность по позже можно будет перезвонить всего доброго до свидания\n",
      "['застраховать', 'лицо', 'хотеть', 'узнать', 'договор', 'полис', 'дата', 'оплата', 'страховой', 'премия', 'повод', 'кража', 'украсть', 'тысяча', 'мошенник', 'страховой', 'написать', 'заявление', 'подать', 'страховой', 'случай', 'правильно', 'понимать', 'должный', 'указать', 'договор', 'полис', 'телефон', 'весь', 'оплачивать', 'договор', 'оплатить', 'правильно', 'понимать', 'мошенник', 'снять', 'тысяча', 'хотеть', 'страховой', 'служба', 'написать', 'заявление', 'указать', 'договор', 'полис', 'дата', 'оплата', 'страховой', 'премия', 'оплачиваться', 'нужный', 'номер', 'договор', 'номер', 'договор', 'сообщать', 'заявление', 'повод', 'кража', 'сказать', 'звонить', 'сбербанк', 'щас', 'сидеть', 'деньга', 'позвонить', 'послать', 'обратиться', 'номер', 'договор', 'номер', 'договор', 'говорить', 'сообщать', 'минуточка', 'подождать', 'сообщать', 'делать', 'документация', 'ладный', 'минуточка', 'подождать', 'выяснить', 'сказать', 'уточнить', 'указать', 'номер', 'договор', 'заявление', 'наступление', 'событие', 'иметь', 'признак', 'страховой', 'случай', 'сбербанк', 'страхование', 'технический', 'причина', 'отсутствовать', 'возможность', 'предоставить', 'интересовать', 'информация', 'приносить', 'извинение', 'доставить', 'неудобство', 'приносить', 'просить', 'перезвонить', 'понять', 'слово', 'технический', 'причина', 'отсутствовать', 'возможность', 'предоставить', 'интересовать', 'информация', 'приносить', 'извинение', 'доставить', 'неудобство', 'просить', 'перезвонить', 'поздний', 'сбербанк', 'честно', 'говорить', 'час', 'допустить', 'заявление', 'технический', 'возможность', 'предоставить', 'информация', 'поздний', 'перезвонить', 'технический', 'возможность', 'поздний', 'перезвонить']\n",
      "--------------\n",
      "Куда  #ТОПОНИМ  мой аватар Ну если не понял то позови оператора !  #ФИО  , где шаблоны операций и мой аватар , который всегда был Здравствуйте! Проверяю, скоро вернусь Войдите в приложение сегодня после  #ЧИСЛО : #ЧИСЛО  по МСК, сейчас временные ограничения\n",
      "['аватар', 'понять', 'позвать', 'оператор', 'шаблон', 'операция', 'аватар', 'который', 'проверять', 'скоро', 'вернуться', 'войти', 'приложение', 'сегодня', 'мск', 'временной', 'ограничение']\n",
      "--------------\n",
      "меня зовут  #ФИО  здравствуйте здравствуйте у меня вопрос такого характера сбербанк онлайн открываю мне нужна кредитная история и почему то не пишет пишет на портале у вас нет доступа к данной операции выступил данную операцию сбербанк онлайн на телефоне или на компьютере  #ФИО   #ФИО  на компьютере на компьютере пару минут ожидайте спасибо за ожидание да действительно сейчас недоступна по той причине что идут технологические работы по вашему сбербанк онлайн так должны закончится после  #ЧИСЛО  часов сегодняшнего дня по  #ТОПОНИМ  все понятно я то думала адресная какая то отказ доступа именно мне спасибо угу спасибо всего доброго угу до свидания\n",
      "['звать', 'вопрос', 'характер', 'сбербанк', 'онлайн', 'открывать', 'нужный', 'кредитный', 'история', 'писать', 'писать', 'портал', 'доступ', 'дать', 'операция', 'выступить', 'данный', 'операция', 'сбербанк', 'онлайн', 'телефон', 'компьютер', 'компьютер', 'компьютер', 'пара', 'ожидать', 'ожидание', 'действительно', 'недоступный', 'причина', 'идти', 'технологический', 'работа', 'ваш', 'сбербанк', 'онлайн', 'должный', 'закончиться', 'часы', 'сегодняшний', 'весь', 'думать', 'адресный', 'отказ', 'доступ']\n",
      "--------------\n",
      " #ФИО  здравствуйте добрый день я вас слушаю я вот захожу в свой сбербанк приложение и какая то код ошибки  #ЧИСЛО  ноль один времени что то недоступно не работает и сказали позвонить в этот на вам  #ФИО  минута но к сожалению сейчас ведутся технические ра просим вас перезайти в данное приложение после  #ЧИСЛО  вечера по московскому времени ага хорошо понял всего доброго до свидания спасибо до свидания\n",
      "['слушать', 'заходить', 'свой', 'сбербанк', 'приложение', 'код', 'ошибка', 'ноль', 'время', 'недоступный', 'работать', 'сказать', 'позвонить', 'вестись', 'технический', 'просить', 'перезайти', 'данный', 'приложение', 'московский', 'время', 'понять']\n",
      "--------------\n",
      "меня зовут  #ФИО  здравствуйте  #ФИО  добрый день вы знаете хочу вас спросить вот сегодня с утра делал переводы в сбербанке онлайн и почему то в истории я переводила за автоплатеж доплачивала а в истории они не отображаются хотя деньги с карты ушли поняла вас минуту сейчас все проверим и все я увижу да  #ФИО   #ФИО  а информация не отражена в истории в связи с тем что на данный момент проводятся технологические работы в личном кабинете они будут завершены после  #ЧИСЛО  часов по  #ТОПОНИМ  в это время сможете проверить конечно все спасибо большое тоже перепугалась ага спасибо до свидания не переживайте хорошего дня\n",
      "['звать', 'хотеть', 'спросить', 'сегодня', 'делать', 'перевод', 'сбербанк', 'онлайн', 'история', 'переводить', 'автоплатёж', 'доплачивать', 'история', 'отображаться', 'деньга', 'карта', 'уйти', 'понять', 'весь', 'проверить', 'весь', 'увидеть', 'информация', 'отразить', 'история', 'связь', 'данный', 'момент', 'проводиться', 'технологический', 'работа', 'личный', 'кабинет', 'завершить', 'часы', 'время', 'смочь', 'проверить', 'весь', 'большой', 'перепугаться', 'переживать', 'хороший']\n"
     ]
    }
   ],
   "source": [
    "for i, row in df[['msg', 'msg_proc']].head().iterrows():\n",
    "    print(\"--------------\")\n",
    "    print(f\"{row['msg']}\\n{row['msg_proc']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bigrammer:\n",
    "    def __init__(self, phrase_model=None):\n",
    "        self.__phrase_model = phrase_model\n",
    "    \n",
    "    def train(self, texts: List[List[str]], \n",
    "              min_count: int, threshold: float,\n",
    "             to_save: bool, save_path: str, phrases_fn: str):\n",
    "        self.__phrase_model = Phrases(texts, min_count=min_count, \n",
    "                               threshold=threshold, scoring='npmi')\n",
    "        if to_save:\n",
    "            self.__phrase_model.save(os.path.join(save_path, phrases_fn))\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, save_path: str, phrases_fn: str) -> object:\n",
    "        \"\"\"\n",
    "        Load pre-trained model from file and init.\n",
    "        \"\"\"\n",
    "        phrase_model = Phrases.load(os.path.join(save_path, phrases_fn))\n",
    "        return cls(phrase_model=phrase_model)\n",
    "    \n",
    "    \n",
    "    def create_bigramms(self, texts: List[List[str]], max_len: int=150) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Create bi-gramms from given text data, already splitted.\n",
    "        \"\"\"\n",
    "        return [self.__phrase_model[text] if len(text) > 0 else np.zeros((max_len,)) for text in texts]\n",
    "    \n",
    "    \n",
    "    def get_vocab(self):\n",
    "        return self.__phrase_model.vocab\n",
    "    \n",
    "    \n",
    "    def get_phraser(self):\n",
    "        return self.__phrase_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRanker:\n",
    "    \"\"\"\n",
    "    TextRank for keyword extraction.\n",
    "    This model builds a graph that represents the text. A graph based ranking\n",
    "    algorithm is then applied to extract the lexical units (here the words) that\n",
    "    are most important in the text.\n",
    "    In this implementation, \n",
    "     - nodes - are words of certain part-of-speech (nouns/adjectives/..) \n",
    "     - edges - represent co-occurrence relation, controlled by the distance \n",
    "               between word occurrences - a window of N words). \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ):\n",
    "        # Russian language parser\n",
    "        self.__morph = pymorphy2.MorphAnalyzer()\n",
    "        # Words graph\n",
    "        self.__graph = nx.Graph()\n",
    "        self.__texts = []  # as List[Dict[str, Any]] -> [{'words': [], 'POS': []}]\n",
    "        # Each inner Dict == single text\n",
    "        # Outer List is composition of texts\n",
    "        \n",
    "    def candidate_weighting(self, texts: List[List[str]],\n",
    "                            window: int=2, pos_list: List[str]=None,\n",
    "                            include_bigramms: bool=True,\n",
    "                            top_percent: float=None):\n",
    "        \"\"\"\n",
    "        Tailored candidate ranking method for TextRank. \n",
    "        Keyphrase candidates are either composed from the T-percent (top_percent) \n",
    "        highest-ranked words or extracted using the `candidate_selection()` method.\n",
    "        Candidates are ranked using the sum of their words.\n",
    "        :param window - the window for connecting words in the graph.\n",
    "        :param pos_list - the set of valid pos for words to be considered as nodes\n",
    "                    in the graph, defaults to ('NOUN', 'PROPN', 'ADJ').\n",
    "        :param top_percent - percentage of top vertices to keep for phrase generation.\n",
    "        \"\"\"\n",
    "        self.__texts = texts\n",
    "        self.__window = window\n",
    "        self.__include_bigramms = include_bigramms\n",
    "        if (pos_list is None) and ~include_bigramms:\n",
    "            # From pymorphy2 avaliable POS tags\n",
    "            # ref: http://opencorpora.org/dict.php?act=gram \n",
    "            self.__pos_list = ['NOUN', 'ADJS', 'ADJF', 'COMP', 'VERB', 'INFN', \n",
    "                               'PRTF', 'PRTS', 'GRND', 'NUMR', 'ADVB', 'Abbr']\n",
    "        else:\n",
    "            self.__pos_list = None\n",
    "            \n",
    "        self.__build_word_graph()\n",
    "        \n",
    "    def __tag_words(self, texts):\n",
    "        return [{'tokens': tokens, \n",
    "                 'pos': [self.__morph.parse(str(token).lower())[0].tag \n",
    "                         for token in text]} \n",
    "                for tokens in tqdm_notebook(texts)]\n",
    "        \n",
    "            \n",
    "            \n",
    "    def __build_word_graph(self):\n",
    "        \"\"\"\n",
    "        Build a graph representation of the document in which nodes/vertices\n",
    "        are words and edges represent co-occurrence relation. Syntactic filters\n",
    "        can be applied to select only words of certain Part-of-Speech.\n",
    "        Co-occurrence relations can be controlled using the distance between\n",
    "        word occurrences in the document.\n",
    "        \"\"\"\n",
    "        if ~(pos_list is None) and ~include_bigramms:\n",
    "            # flatten document as a sequence of (word, pass_syntactic_filter) tuples\n",
    "            text = [(word, sentence.pos[i] in pos) for sentence in self.sentences\n",
    "                    for i, word in enumerate(self.__texts)]\n",
    "\n",
    "        # add nodes to the graph\n",
    "        self.graph.add_nodes_from([word for word, valid in text if valid])\n",
    "\n",
    "        # add edges to the graph\n",
    "        for i, (node1, is_in_graph1) in enumerate(text):\n",
    "\n",
    "            # speed up things\n",
    "            if not is_in_graph1:\n",
    "                continue\n",
    "\n",
    "            for j in range(i + 1, min(i + window, len(text))):\n",
    "                node2, is_in_graph2 = text[j]\n",
    "                if is_in_graph2 and node1 != node2:\n",
    "                    self.graph.add_edge(node1, node2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "morph.parse(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
