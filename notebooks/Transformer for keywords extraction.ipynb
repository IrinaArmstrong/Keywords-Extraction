{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import gc\n",
    "import pickle\n",
    "import psutil\n",
    "import psycopg2\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict, namedtuple, deque, Counter\n",
    "from typing import (List, Dict, Any, NoReturn, \n",
    "                    Tuple, Optional, Union)\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime as dt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import logging\n",
    "import multiprocessing\n",
    "from multiprocessing_logging import install_mp_handler\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W2V\n",
    "import spacy\n",
    "import pymorphy2\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.phrases import Phrases\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim.models.phrases import Phrases, npmi_scorer\n",
    "from gensim.models import word2vec, keyedvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"rm\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0721 15:20:22.686534 101160 tpu_cluster_resolver.py:35] Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n",
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "# Keras NN\n",
    "%load_ext tensorboard\n",
    "!rm -rf ./logs/\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD, schedules\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# Keras Addons\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow_addons.losses import metric_learning\n",
    "from tensorflow_addons.utils.types import FloatTensorLike, TensorLike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System & Devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU count: 12\n",
      "CPU utilization: 13.7%\n",
      "Memory total: 16235.328125 MB.\n",
      "Memory available: 7668.484375 MB.\n",
      "Memory used: 8566.84375 MB.\n"
     ]
    }
   ],
   "source": [
    "print(f\"CPU count: {psutil.cpu_count()}\")\n",
    "print(f\"CPU utilization: {psutil.cpu_percent()}%\")\n",
    "mem_stats = psutil.virtual_memory()\n",
    "print(f\"Memory total: {mem_stats.total / 1048576} MB.\")\n",
    "print(f\"Memory available: {mem_stats.available / 1048576} MB.\")\n",
    "print(f\"Memory used: {mem_stats.used / 1048576} MB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:XLA_CPU:0', '/device:XLA_GPU:0']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 7232031686440726178,\n",
       " name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 11055582075212373904\n",
       " physical_device_desc: \"device: XLA_CPU device\",\n",
       " name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 589692531033251638\n",
       " physical_device_desc: \"device: XLA_GPU device\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "print(get_available_devices()) \n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path(\"..\")\n",
    "DATA_DIR  = BASE_DIR / \"data\"\n",
    "MODEL_DIR  = BASE_DIR / \"models\"\n",
    "CHKP_DIR = MODEL_DIR / \"chkp_dir\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors_data(start_date: str, end_date: str, is_prediction: Tuple[bool], \n",
    "                      last_updated: str, maxnum: int=1e6):\n",
    "    \n",
    "    print(f\"Creating connection to DB...\")\n",
    "    connection = psycopg2.connect(\"dbname='{}' user='{}' host='{}' password='{}' port = {}\".format(\n",
    "        'pgarmat', 'postgres', 'tklis-sm0000002.vm.mos.cloud.sbrf.ru', 'ZxqZo8lX', '5432'))\n",
    "    print(f\"Connection created.\")\n",
    "    \n",
    "    with connection.cursor() as cursor:\n",
    "        try:\n",
    "            cursor.execute(f\"\"\"\n",
    "                select\n",
    "                req_num, \n",
    "                req_reg_datetime, \n",
    "                msg,\n",
    "                product, \n",
    "                subject,\n",
    "                s_subject, \n",
    "                subproduct,\n",
    "                client_msg_vector,\n",
    "                update_datetime\n",
    "                from ds70.request_messages\n",
    "                where 1=1\n",
    "                and client_msg_vector is not null\n",
    "                and is_prediction in {is_prediction}\n",
    "                and req_reg_datetime between '{start_date}' and '{end_date}'\n",
    "                and update_datetime >= '{last_updated}'\n",
    "                limit {maxnum};\"\"\")\n",
    "            requests = cursor.fetchall()\n",
    "        except Exception as ee:\n",
    "            print(ee)\n",
    "            connection.rollback()\n",
    "            \n",
    "        data = pd.DataFrame(requests, columns=[\"req_num\",\n",
    "                                               \"req_reg_datetime\",\n",
    "                                               \"msg\",\n",
    "                                               \"product\",\n",
    "                                               \"subject\",\n",
    "                                               \"s_subject\",\n",
    "                                               \"subproduct\",\n",
    "                                               \"client_msg_vector\",\n",
    "                                               \"update_datetime\"])\n",
    "        print(f\"Loaded data shape: {data.shape} and size: {sys.getsizeof(data) / 1048576} MB.\")\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_vectors_data(start_date='2021-02-01 0:00:01', end_date='2021-02-10 23:59:59', \n",
    "                       is_prediction=(True, False), last_updated='2021-07-19', maxnum=600_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label_name'] = df['product'] + \" : \" + df['s_subject']\n",
    "print(df['label_name'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select most frequent classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_labels = df.label_name.value_counts().reset_index().rename({'index': \"label_name_name\"}, axis=1)\n",
    "print(freq_labels.describe())\n",
    "\n",
    "freq_labels.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only most frequent subjects\n",
    "n_freq_labels = 50\n",
    "\n",
    "freq_labels = freq_labels.loc[freq_labels.label_name > n_freq_labels]\n",
    "print(\"Lables left: \", freq_labels.shape[0])\n",
    "freq_labels_list = freq_labels.label_name_name.to_list()\n",
    "\n",
    "df = df.loc[df.label_name.isin(freq_labels_list)]\n",
    "print(f\"Left {len(df)} requests.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.3, \n",
    "                                     random_state=11, stratify=df.label_name)\n",
    "test_df, val_df = train_test_split(test_df, test_size=0.4, \n",
    "                                   random_state=11, stratify=test_df.label_name)\n",
    "print(f\"Train data: {train_df.shape[0]} test data: {test_df.shape[0]}, val data: {val_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataVectorizer:\n",
    " \n",
    "    def __init__(self, phrase_model: Phrases = None,\n",
    "                 w2v_vectorizer: Optional[KeyedVectors] = None):\n",
    "        \"\"\"\n",
    "        Need to load pre-trained models from files.\n",
    "        \"\"\"\n",
    "        self._w2v = w2v_vectorizer\n",
    "        self._phrase_model = phrase_model\n",
    "        self._vocab_size, self._embedding_size = self._w2v.vectors.shape\n",
    " \n",
    "       \n",
    "    @classmethod\n",
    "    def load(cls, save_path: str, phrases_fn: str, w2v_fn: str) -> object:\n",
    "        \"\"\"\n",
    "        Load pre-trained models from files and init.\n",
    "        \"\"\"\n",
    "        phrase_model = Phrases.load(os.path.join(save_path, phrases_fn))\n",
    "        w2v_vectorizer = KeyedVectors.load_word2vec_format(os.path.join(save_path, w2v_fn), binary=False)\n",
    "        return cls(phrase_model=phrase_model,\n",
    "                   w2v_vectorizer=w2v_vectorizer)\n",
    "   \n",
    "    \n",
    "    def _word2id(self, token_seq: List[List[str]], max_len: int):\n",
    "        \"\"\"\n",
    "        Convert token to id in W2V vocabulary.\n",
    "        \"\"\"\n",
    "        id_seq = [[self._w2v.get_index(tok) + 1 for tok in doc if self._w2v.has_index_for(tok)]\n",
    "                  for doc in tqdm(token_seq)]\n",
    "#         id_seq = pad_sequences(id_seq, value=0, maxlen=max_len, padding='post', truncating='post')\n",
    "        return id_seq\n",
    " \n",
    "   \n",
    "    def vectorize(self, data: List[str], max_len: int) -> List[int]:\n",
    "        \"\"\"\n",
    "        Vectorize data:\n",
    "            - encode words as ids\n",
    "            - reduce dimensions\n",
    "        \"\"\"\n",
    "        data = [self._phrase_model[d] if len(d) > 0 else np.zeros((max_len,)) for d in data]\n",
    "        data = self._word2id(data, max_len)\n",
    "        return data\n",
    "   \n",
    "    def get_vectors(self):\n",
    "        return self._w2v.vectors\n",
    "    \n",
    "    \n",
    "dvect = DataVectorizer.load(str(MODEL_DIR), phrases_fn=\"phrases_gensim_lemmas_12_07_2021.pkl\",\n",
    "                           w2v_fn='word2vec_gensim_lemmas_12_07_2021')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder().fit(train_df.label_name)\n",
    "train_df['label'] = le.transform(train_df.label_name)\n",
    "test_df['label'] = le.transform(test_df.label_name)\n",
    "val_df['label'] = le.transform(val_df.label_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_df['label'].values\n",
    "test_labels = test_df['label'].values\n",
    "val_labels = val_df['label'].values\n",
    "\n",
    "train_vectors = pad_sequences(train_df['client_msg_vector'].to_list(), value=0, maxlen=150, padding='post', truncating='post')\n",
    "test_vectors = pad_sequences(test_df['client_msg_vector'].to_list(), value=0, maxlen=150, padding='post', truncating='post')\n",
    "val_vectors = pad_sequences(val_df['client_msg_vector'].to_list(), value=0, maxlen=150, padding='post', truncating='post')\n",
    "\n",
    "train_indexes = [i for i, vec in enumerate(train_vectors) if np.count_nonzero(vec) > 19]\n",
    "test_indexes = [i for i, vec in enumerate(test_vectors) if np.count_nonzero(vec) > 19]\n",
    "val_indexes = [i for i, vec in enumerate(val_vectors) if np.count_nonzero(vec) > 19]\n",
    "\n",
    "train_vectors, train_labels = train_vectors[train_indexes], train_labels[train_indexes]\n",
    "test_vectors, test_labels = test_vectors[test_indexes], test_labels[test_indexes]\n",
    "val_vectors, val_labels = val_vectors[val_indexes], val_labels[val_indexes]\n",
    "\n",
    "train_vectors = np.vstack(train_vectors)\n",
    "test_vectors = np.vstack(test_vectors)\n",
    "val_vectors = np.vstack(val_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    \"\"\"\n",
    "    Two seperate embedding layers, one for tokens, one for token index (positions).\n",
    "    \"\"\"\n",
    "    def __init__(self, maxlen: int, vocab_size: int, \n",
    "                 embedding_size: int, pretrained_weights: np.ndarray):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self._token_emb = layers.Embedding(input_dim=vocab_size + 1, \n",
    "                                           output_dim=embedding_size,\n",
    "                                           weights=[np.vstack((np.zeros((1, embedding_size), dtype=np.float32), \n",
    "                                                    pretrained_weights))], input_length=maxlen,\n",
    "                                          mask_zero=True, trainable=True)\n",
    "        self._pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embedding_size)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self._pos_emb(positions)\n",
    "        x = self._token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    \n",
    "    def __init__(self, embedding_size: int, num_heads: int, out_dim: int, dropout=0.5):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        # If query, key, value are the same, then this is self-attention. \n",
    "        # Each timestep in query attends to the corresponding sequence in key, and returns a fixed-width vector.\n",
    "        self._att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_size)\n",
    "        self._ffn = Sequential([\n",
    "             layers.Dense(out_dim, activation=\"relu\"), \n",
    "             layers.Dense(embedding_size)\n",
    "        ])\n",
    "        self._layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self._layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self._dropout1 = layers.Dropout(dropout)\n",
    "        self._dropout2 = layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, inputs, is_training: bool):\n",
    "        # query, value, [key] optional == value\n",
    "        # attention_output of shape [B, T, E] (if return_attention_scores=False)\n",
    "        attn_output = self._att(inputs, inputs)\n",
    "        attn_output = self._dropout1(attn_output, training=is_training)\n",
    "        out1 = self._layernorm1(inputs + attn_output)\n",
    "        ffn_output = self._ffn(out1)\n",
    "        ffn_output = self._dropout2(ffn_output, training=is_training)\n",
    "        return self._layernorm2(out1 + ffn_output), attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel():\n",
    "    \n",
    "    def __init__(self, num_heads: int,\n",
    "                 input_length: int, embedding_size: int, \n",
    "                 inner_dims: int, out_dims: int, vocab_size: int,\n",
    "                 pretrained_weights: np.ndarray, dropout: float=0.5, \n",
    "                 pretrained=False, model_weights_path='./model.hdf5'):\n",
    "        \n",
    "        self._num_heads = num_heads  # Number of attention heads\n",
    "        self._input_length = input_length  # Number of tokens in text (~150)\n",
    "        self._pretrained_weights = pretrained_weights\n",
    "        assert(self._pretrained_weights.shape[0] == vocab_size, \n",
    "               f\"Pretrained weights size and vocabulary size should be equal!\")\n",
    "        self._vocab_size = vocab_size  # Embeddings vocab size\n",
    "        assert(self._pretrained_weights.shape[1] == embedding_size, \n",
    "               f\"Pretrained weights size and embedding size should be equal!\")\n",
    "        self._embedding_size = embedding_size  # Embedding size for each token\n",
    "        self._out_dims = out_dims  # Output text embedding vector size\n",
    "        self._inner_dims = inner_dims # Hidden layer size in feed forward network inside transformer\n",
    "        self._dropout = dropout\n",
    "        self._model, self._attention_model = self.create_network()\n",
    "        if pretrained:\n",
    "            self.load_pretrained(model_weights_path)\n",
    "            \n",
    "    def create_network(self):\n",
    "        input_layer = layers.Input((self._input_length,))\n",
    "        embedding_layer = TokenAndPositionEmbedding(maxlen=self._input_length, \n",
    "                                                    vocab_size=self._vocab_size, \n",
    "                                                    embedding_size=self._embedding_size,\n",
    "                                                    pretrained_weights=self._pretrained_weights)(input_layer)\n",
    "        transformer_block = TransformerBlock(embedding_size=self._embedding_size, \n",
    "                                             num_heads=self._num_heads, \n",
    "                                             out_dim=self._inner_dims, \n",
    "                                             dropout=self._dropout)\n",
    "        transformer_out, transformer_att = transformer_block(embedding_layer)\n",
    "        avgpool = layers.GlobalAveragePooling1D()(transformer_out)\n",
    "        drop = layers.Dropout(0.1)(avgpool)\n",
    "        dense = layers.Dense(self._out_dims, activation=\"relu\")(drop)\n",
    "        output_layer = Lambda(lambda x: tf.math.l2_normalize(x, axis=1))(dense)\n",
    "        att_model = Model(input_layer, [output_layer, transformer_att])\n",
    "        model = Model(input_layer, output_layer)\n",
    "        return model, att_model\n",
    "            \n",
    "    def load_pretrained_model(self, model_weights_path: str):\n",
    "        self._model.load_weights(model_weights_path)\n",
    "        \n",
    "    def load_pretrained_attention_model(self, model_weights_path: str):\n",
    "        self._attention_model.load_weights(model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.keras.layers' has no attribute 'MultiHeadAttention'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-20bda7587d50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                         \u001b[0mpretrained_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10_000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m#dvect.get_vectors(),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                          \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                         pretrained=False, model_weights_path=str(MODEL_DIR / \"attention_lstm_50dim_13-07.h5\"))\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_attention_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-15c4f497d822>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, num_heads, input_length, embedding_size, inner_dims, out_dims, vocab_size, pretrained_weights, dropout, pretrained, model_weights_path)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner_dims\u001b[0m \u001b[1;31m# Hidden layer size in feed forward network inside transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dropout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_attention_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_weights_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-15c4f497d822>\u001b[0m in \u001b[0;36mcreate_network\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     32\u001b[0m                                              \u001b[0mnum_heads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_heads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                                              \u001b[0mout_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_dims\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                                              dropout=self._dropout)\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mtransformer_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformer_att\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mavgpool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGlobalAveragePooling1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-5aa04d250e47>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, embedding_size, num_heads, out_dim, dropout)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;31m# If query, key, value are the same, then this is self-attention.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;31m# Each timestep in query attends to the corresponding sequence in key, and returns a fixed-width vector.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_att\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultiHeadAttention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         self._ffn = Sequential([\n\u001b[0;32m      9\u001b[0m              \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.keras.layers' has no attribute 'MultiHeadAttention'"
     ]
    }
   ],
   "source": [
    "model = TransformerModel(num_heads=2, input_length=150, embedding_size=200,  #dvect._embedding_size, \n",
    "                        inner_dims=128, out_dims=50, vocab_size=10_000,  #dvect._vocab_size,\n",
    "                        pretrained_weights=np.zeros((10_000, 200), dtype=float),  #dvect.get_vectors(), \n",
    "                         dropout=0.6,\n",
    "                        pretrained=False, model_weights_path=str(MODEL_DIR / \"attention_lstm_50dim_13-07.h5\"))\n",
    "\n",
    "model._attention_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
